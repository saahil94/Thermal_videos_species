{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 clips processed!\n",
      "200 clips processed!\n",
      "300 clips processed!\n",
      "400 clips processed!\n",
      "500 clips processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 clips processed!\n",
      "700 clips processed!\n",
      "800 clips processed!\n",
      "900 clips processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in subtract\n",
      "C:\\Users\\Saahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in half_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 clips processed!\n",
      "1100 clips processed!\n",
      "1200 clips processed!\n",
      "1300 clips processed!\n",
      "1400 clips processed!\n",
      "1500 clips processed!\n",
      "1600 clips processed!\n",
      "1700 clips processed!\n",
      "1800 clips processed!\n",
      "1900 clips processed!\n",
      "2000 clips processed!\n",
      "2100 clips processed!\n",
      "2200 clips processed!\n",
      "2300 clips processed!\n",
      "2400 clips processed!\n",
      "2500 clips processed!\n",
      "2600 clips processed!\n",
      "2700 clips processed!\n",
      "2800 clips processed!\n",
      "2900 clips processed!\n",
      "3000 clips processed!\n",
      "3100 clips processed!\n",
      "3200 clips processed!\n",
      "3300 clips processed!\n",
      "3400 clips processed!\n",
      "3500 clips processed!\n",
      "3600 clips processed!\n",
      "3700 clips processed!\n",
      "3800 clips processed!\n",
      "3900 clips processed!\n",
      "4000 clips processed!\n",
      "4100 clips processed!\n",
      "4200 clips processed!\n",
      "4300 clips processed!\n",
      "4400 clips processed!\n",
      "4500 clips processed!\n",
      "4600 clips processed!\n",
      "4700 clips processed!\n",
      "4800 clips processed!\n",
      "4900 clips processed!\n",
      "5000 clips processed!\n",
      "5100 clips processed!\n",
      "5200 clips processed!\n",
      "5300 clips processed!\n",
      "5400 clips processed!\n",
      "5500 clips processed!\n",
      "5600 clips processed!\n",
      "5700 clips processed!\n",
      "5800 clips processed!\n",
      "5900 clips processed!\n",
      "6000 clips processed!\n",
      "6100 clips processed!\n",
      "6200 clips processed!\n",
      "6300 clips processed!\n",
      "6400 clips processed!\n",
      "6500 clips processed!\n",
      "6600 clips processed!\n",
      "6700 clips processed!\n",
      "6800 clips processed!\n",
      "6900 clips processed!\n",
      "7000 clips processed!\n",
      "7100 clips processed!\n",
      "7200 clips processed!\n",
      "7300 clips processed!\n",
      "7400 clips processed!\n",
      "7500 clips processed!\n",
      "7600 clips processed!\n",
      "7700 clips processed!\n",
      "7800 clips processed!\n",
      "7900 clips processed!\n",
      "8000 clips processed!\n",
      "8100 clips processed!\n",
      "8200 clips processed!\n",
      "8300 clips processed!\n",
      "8400 clips processed!\n",
      "8500 clips processed!\n",
      "8600 clips processed!\n",
      "8700 clips processed!\n",
      "8800 clips processed!\n",
      "8900 clips processed!\n",
      "9000 clips processed!\n",
      "9100 clips processed!\n",
      "9200 clips processed!\n",
      "9300 clips processed!\n",
      "9400 clips processed!\n",
      "9500 clips processed!\n",
      "9600 clips processed!\n",
      "9700 clips processed!\n",
      "9800 clips processed!\n",
      "9900 clips processed!\n",
      "10000 clips processed!\n",
      "10100 clips processed!\n",
      "10200 clips processed!\n",
      "10300 clips processed!\n",
      "10400 clips processed!\n",
      "10500 clips processed!\n",
      "10600 clips processed!\n"
     ]
    }
   ],
   "source": [
    "import h5py    \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Discards videos missing a usable tag.\n",
    "Discards videos with fewer than 45 frames.\n",
    "Trims the length of the videos to 45 frames.\n",
    "Interpolates each of the cropped frames to 24 x 24.\n",
    "Outputs 3 channels:\n",
    "    (1) The raw thermal values (min-max normalization)\n",
    "    (2) The raw thermal values (each frame normalized independently)\n",
    "    (3) The thermal values minus the background (min-max normalization)\n",
    "Splits the data into training, validation, and test sets.\n",
    "Encodes the labels as integers.\n",
    "Saves the pre-processed data and the labels as numpy arrays.\n",
    "\"\"\"\n",
    "\n",
    "validation_num = 1500\n",
    "test_num = 1500\n",
    "\n",
    "f = h5py.File(\"C:/Users/Saahil/OneDrive/Documents/COMPSCI 760/Research Project/dataset.hdf5\", \"r\") # Read in the dataset\n",
    "d = f[list(f.keys())[0]]                                        # Access the thermal videos key\n",
    "clips = np.zeros([10664, 45, 3, 24, 24], dtype=np.float16)      # np.float16 saves storage space\n",
    "\n",
    "def get_best_index(vid):\n",
    "    \"\"\"\n",
    "    Returns an index such that the selected 45 frames from a given video correspond to\n",
    "    the 45 frames where the animal is nearest to the camera.\n",
    "    \"\"\"\n",
    "    mass = np.zeros(vid.attrs['frames'])\n",
    "    for f in range(vid.attrs['frames']):\n",
    "        mass[f] = np.sum(vid[str(f)][4])\n",
    "    total_mass_over_next_45 = np.cumsum(mass) - np.hstack([np.zeros(45), np.cumsum(mass[:-45])])\n",
    "    return f - np.argmax(total_mass_over_next_45[::-1]) - 44\n",
    "\n",
    "def make24x24(frame):\n",
    "    \"\"\"\n",
    "    Interpolates a given frame so its largest dimension is 24. The padding uses the minimum\n",
    "    of the frame's values across each channel.\n",
    "    \"\"\"\n",
    "    scale = (24.5 / np.array(frame.shape[1:])).min()\n",
    "    frame = torch.tensor(np.expand_dims(frame, 0))\n",
    "    frame = np.array(nn.functional.interpolate(frame, scale_factor = scale, mode = 'area')[0])\n",
    "    square = np.tile(np.min(frame, (1, 2)).reshape(3, 1, 1), (1, 24, 24))\n",
    "    offset = ((np.array([24, 24]) - frame.shape[1:]) / 2).astype(np.int)\n",
    "    square[:, offset[0] : offset[0]+frame.shape[1], offset[1] : offset[1]+frame.shape[2]] = frame\n",
    "    return square\n",
    "\n",
    "def normalize(frame):\n",
    "    \"\"\"\n",
    "    Min-max normalizes the first channel (clipping outliers).\n",
    "    Min-max normalizes the second channel for each frame independently.\n",
    "    Min-max normalizes the third channel (clipping outliers).\n",
    "    \"\"\"\n",
    "    frame[0] = np.clip((frame[0] - 2500) / 1000, 0, 1)\n",
    "    frame[1] = np.nan_to_num((frame[1] - frame[1].min()) / (frame[1].max() - frame[1].min()))\n",
    "    frame[2] = np.clip(frame[2] / 400, 0, 1)\n",
    "    return frame\n",
    "\n",
    "labels_raw = []\n",
    "processed = 0\n",
    "for i in range(len(d.keys())):\n",
    "    x = d[list(d.keys())[i]]\n",
    "    for j in range(len(x.keys()) - 1):\n",
    "        vid = x[list(x.keys())[j]]\n",
    "        tag = vid.attrs['tag']\n",
    "        if tag == \"bird/kiwi\":\n",
    "            tag = \"bird\"\n",
    "        if vid.attrs['frames'] >= 45 and not tag in [\"unknown\", \"part\", \"poor tracking\", \"sealion\"]:\n",
    "            labels_raw += [tag]\n",
    "            ind = get_best_index(vid)\n",
    "            for f in range(45):\n",
    "                frame = np.array(vid[str(f+ind)], dtype=np.float16)[:2]         # Read a single frame\n",
    "                frame = np.concatenate([np.expand_dims(frame[0], 0), frame], 0) # The desired 3 channels\n",
    "                frame = make24x24(frame)                                        # Interpolate the frame\n",
    "                frame = normalize(frame)                                        # Normalizes each channel\n",
    "                clips[processed, f] = frame\n",
    "            processed += 1                   \n",
    "            if processed % 100 == 0:        \n",
    "                print(processed, \"clips processed!\")\n",
    "\n",
    "# We encode the labels as an integer for each class\n",
    "labels = LabelEncoder().fit_transform(labels_raw)\n",
    "\n",
    "labels_raw = np.array(labels_raw)\n",
    "\n",
    "# We extract the training, test and validation sets, with a fixed random seed for reproducibility and stratification\n",
    "clips, val_vids, labels, val_labels, labels_raw, val_labels_raw = train_test_split(clips, labels, labels_raw, test_size = validation_num, random_state = 123, stratify = labels)\n",
    "train_vids, test_vids, train_labels, test_labels, train_labels_raw, test_labels_raw = train_test_split(clips, labels, labels_raw, test_size = test_num, random_state = 123, stratify = labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save all of the files\n",
    "if not os.path.exists(\"./cacophony-preprocessed\"):\n",
    "    os.makedirs(\"./cacophony-preprocessed\")\n",
    "np.save(\"./cacophony-preprocessed/training\", train_vids)\n",
    "np.save(\"./cacophony-preprocessed/validation\", val_vids)\n",
    "np.save(\"./cacophony-preprocessed/test\", test_vids)\n",
    "np.save(\"./cacophony-preprocessed/training-labels\", train_labels)\n",
    "np.save(\"./cacophony-preprocessed/validation-labels\", val_labels)\n",
    "np.save(\"./cacophony-preprocessed/test-labels\", test_labels)\n",
    "np.save(\"./cacophony-preprocessed/training-labels_raw\", train_labels_raw)\n",
    "np.save(\"./cacophony-preprocessed/validation-labels_raw\", val_labels_raw)\n",
    "np.save(\"./cacophony-preprocessed/test-labels_raw\", test_labels_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
