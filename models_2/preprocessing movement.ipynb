{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 clips processed!\n",
      "200 clips processed!\n",
      "300 clips processed!\n",
      "400 clips processed!\n",
      "500 clips processed!\n",
      "600 clips processed!\n",
      "700 clips processed!\n",
      "800 clips processed!\n",
      "900 clips processed!\n",
      "1000 clips processed!\n",
      "1100 clips processed!\n",
      "1200 clips processed!\n",
      "1300 clips processed!\n",
      "1400 clips processed!\n",
      "1500 clips processed!\n",
      "1600 clips processed!\n",
      "1700 clips processed!\n",
      "1800 clips processed!\n",
      "1900 clips processed!\n",
      "2000 clips processed!\n",
      "2100 clips processed!\n",
      "2200 clips processed!\n",
      "2300 clips processed!\n",
      "2400 clips processed!\n",
      "2500 clips processed!\n",
      "2600 clips processed!\n",
      "2700 clips processed!\n",
      "2800 clips processed!\n",
      "2900 clips processed!\n",
      "3000 clips processed!\n",
      "3100 clips processed!\n",
      "3200 clips processed!\n",
      "3300 clips processed!\n",
      "3400 clips processed!\n",
      "3500 clips processed!\n",
      "3600 clips processed!\n",
      "3700 clips processed!\n",
      "3800 clips processed!\n",
      "3900 clips processed!\n",
      "4000 clips processed!\n",
      "4100 clips processed!\n",
      "4200 clips processed!\n",
      "4300 clips processed!\n",
      "4400 clips processed!\n",
      "4500 clips processed!\n",
      "4600 clips processed!\n",
      "4700 clips processed!\n",
      "4800 clips processed!\n",
      "4900 clips processed!\n",
      "5000 clips processed!\n",
      "5100 clips processed!\n",
      "5200 clips processed!\n",
      "5300 clips processed!\n",
      "5400 clips processed!\n",
      "5500 clips processed!\n",
      "5600 clips processed!\n",
      "5700 clips processed!\n",
      "5800 clips processed!\n",
      "5900 clips processed!\n",
      "6000 clips processed!\n",
      "6100 clips processed!\n",
      "6200 clips processed!\n",
      "6300 clips processed!\n",
      "6400 clips processed!\n",
      "6500 clips processed!\n",
      "6600 clips processed!\n",
      "6700 clips processed!\n",
      "6800 clips processed!\n",
      "6900 clips processed!\n",
      "7000 clips processed!\n",
      "7100 clips processed!\n",
      "7200 clips processed!\n",
      "7300 clips processed!\n",
      "7400 clips processed!\n",
      "7500 clips processed!\n",
      "7600 clips processed!\n",
      "7700 clips processed!\n",
      "7800 clips processed!\n",
      "7900 clips processed!\n",
      "8000 clips processed!\n",
      "8100 clips processed!\n",
      "8200 clips processed!\n",
      "8300 clips processed!\n",
      "8400 clips processed!\n",
      "8500 clips processed!\n",
      "8600 clips processed!\n",
      "8700 clips processed!\n",
      "8800 clips processed!\n",
      "8900 clips processed!\n",
      "9000 clips processed!\n",
      "9100 clips processed!\n",
      "9200 clips processed!\n",
      "9300 clips processed!\n",
      "Clip missing info, imputing with mean.\n",
      "9400 clips processed!\n",
      "9500 clips processed!\n",
      "9600 clips processed!\n",
      "9700 clips processed!\n",
      "9800 clips processed!\n",
      "9900 clips processed!\n",
      "10000 clips processed!\n",
      "10100 clips processed!\n",
      "10200 clips processed!\n",
      "10300 clips processed!\n",
      "10400 clips processed!\n",
      "10500 clips processed!\n",
      "10600 clips processed!\n"
     ]
    }
   ],
   "source": [
    "import h5py    \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Discards videos missing a usable tag.\n",
    "Discards videos with fewer than 45 frames.\n",
    "Collates information of the movement of the cropped region.\n",
    "Outputs 9 variables for each of the 45 frames:\n",
    "    (1) Left boundary of cropped region\n",
    "    (2) Upper boundary of cropped region\n",
    "    (3) Right boundary of cropped region\n",
    "    (4) Lower boundary of cropped region\n",
    "    (5) Number of pixels above a temperature threshold (mass)\n",
    "    (6) Cropped region horizontal velocity\n",
    "    (7) Cropped region vertical velocity\n",
    "    (8) Horizontal velocity scaled by area of cropped region\n",
    "    (9) Vertical velocity scaled by area of cropped region\n",
    "Normalizes each of the 9 variables.\n",
    "Splits the data into training, validation, and test sets.\n",
    "Encodes the labels as integers.\n",
    "Saves the pre-processed data and the labels as numpy arrays.\n",
    "\"\"\"\n",
    "\n",
    "validation_num = 1500\n",
    "test_num = 1500\n",
    "\n",
    "f = h5py.File(\"C:/Users/Saahil/OneDrive/Documents/COMPSCI 760/Research Project/dataset.hdf5\", \"r\") # Read in the dataset\n",
    "d = f[list(f.keys())[0]]                                        # Access the thermal videos key\n",
    "clips = np.zeros([10664, 45, 9])\n",
    "\n",
    "def get_best_index(vid):\n",
    "    \"\"\"\n",
    "    Returns an index such that the selected 45 frames from a given video correspond to\n",
    "    the 45 frames where the animal is nearest to the camera.\n",
    "    \"\"\"\n",
    "    mass = np.zeros(vid.attrs['frames'])\n",
    "    for f in range(vid.attrs['frames']):\n",
    "        mass[f] = np.sum(vid[str(f)][4])\n",
    "    total_mass_over_next_45 = np.cumsum(mass) - np.hstack([np.zeros(45), np.cumsum(mass[:-45])])\n",
    "    return f - np.argmax(total_mass_over_next_45[::-1]) - 44\n",
    "    \n",
    "labels = []\n",
    "processed = 0\n",
    "for i in range(len(d.keys())):\n",
    "    x = d[list(d.keys())[i]]\n",
    "    for j in range(len(x.keys()) - 1):\n",
    "        vid = x[list(x.keys())[j]]\n",
    "        tag = vid.attrs['tag']\n",
    "        if tag == \"bird/kiwi\":\n",
    "            tag = \"bird\"\n",
    "        if vid.attrs['frames'] >= 45 and not tag in [\"unknown\", \"part\", \"poor tracking\", \"sealion\"]:\n",
    "            labels += [tag]\n",
    "            ind = get_best_index(vid)\n",
    "            try:\n",
    "                b_h = vid.attrs['bounds_history'][ind : ind+45]\n",
    "                m_h = vid.attrs['mass_history'][ind : ind+45]\n",
    "                areas = (b_h[:,2] - b_h[:,0]) * (b_h[:,3] - b_h[:,1])\n",
    "                centrex = (b_h[:,2] + b_h[:,0]) / 2\n",
    "                centrey = (b_h[:,3] + b_h[:,1]) / 2\n",
    "                xv = np.hstack((0, centrex[1:] - centrex[:-1]))\n",
    "                yv = np.hstack((0, centrey[1:] - centrey[:-1]))\n",
    "                axv = xv / areas**0.5\n",
    "                ayv = yv / areas**0.5\n",
    "                clips[processed] = np.hstack((b_h, np.vstack((m_h, xv, yv, axv, ayv)).T))\n",
    "            except:\n",
    "                print(\"Clip missing info, imputing with mean.\")\n",
    "                clips[processed] = np.tile([[76, 47, 96, 65, 180, 0.016, -0.015, 0.00076, -0.00055]], (45, 1))\n",
    "                \n",
    "            processed += 1                   \n",
    "            if processed % 100 == 0:        \n",
    "                print(processed, \"clips processed!\")\n",
    "            \n",
    "# Normalizing the data\n",
    "clips -= np.mean(clips, (0,1))\n",
    "clips /= np.std(clips, (0,1))\n",
    "\n",
    "# We encode the labels as an integer for each class\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "\n",
    "# We extract the training, test and validation sets, with a fixed random seed for reproducibility and stratification\n",
    "clips, val_vids, labels, val_labels = train_test_split(clips, labels, test_size = validation_num, random_state = 123, stratify = labels)\n",
    "train_vids, test_vids, train_labels, test_labels = train_test_split(clips, labels, test_size = test_num, random_state = 123, stratify = labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save all of the files\n",
    "if not os.path.exists(\"./cacophony-preprocessed-movement\"):\n",
    "    os.makedirs(\"./cacophony-preprocessed-movement\")\n",
    "np.save(\"./cacophony-preprocessed-movement/training\", train_vids)\n",
    "np.save(\"./cacophony-preprocessed-movement/validation\", val_vids)\n",
    "np.save(\"./cacophony-preprocessed-movement/test\", test_vids)\n",
    "np.save(\"./cacophony-preprocessed-movement/training-labels\", train_labels)\n",
    "np.save(\"./cacophony-preprocessed-movement/validation-labels\", val_labels)\n",
    "np.save(\"./cacophony-preprocessed-movement/test-labels\", test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
